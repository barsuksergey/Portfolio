---
title: "Time Series Analysis "
output: github_document
---

## Analysing time series with R 

The following graph represnets airline passengers each month from 1949 to 1961. This is a pre-loaded dataset in R.

```{r}
data = AirPassengers
plot(data, ylab = "Passengers in thousands", xlab = "Months")
library(forecast)
```

Let's select a lag of 3 (order) and plot our data with the moving average line

```{r}
movavg = ma(data, order = 3)
plot(data)
lines(movavg, lwd = 2, col = "blue")
```

Also we can compare R built in smoothing functions: Holt-Winters and ETS

```{r}
etsdata = ets(data, allow.multiplicative.trend = TRUE)
hwdata = HoltWinters(data, seasonal = "multiplicative")
plot(hwdata$fitted[,1], col = 'red', ylab = 'Smoothed Passangers')
lines(etsdata$fitted, col = 'blue')
```

The smoothing paramers for the models were as follows:

```{r}
etsdata$par[1:3]
c(hwdata$alpha, hwdata$beta, hwdata$gamma)
```

We can also de-seasonalize the graph:

```{r}
stl(data, "periodic") %>% seasadj() %>% plot()
```

Or de-trend to see only seasonal fluctuations:

```{r}
lm(data~c(1:length(data))) %>% resid() %>% plot(type = 'l')
```

We can also predict our values with a 95% confidence interval by using the forecast package with ETS:

```{r}
ff = forecast(data, h = 12, level = 95, allow.multiplicative.trend = TRUE)
ff$method
ff$model$par[1:3]
plot(ff)
accuracy(ff)[5]
```

And Holt-Winters model:

```{r}
fh = hw(data, h = 12, seasonal = "multiplicative", level = 95)
fh$method
fh$model$par[1:3]
plot(fh)
accuracy(fh)[5]
```

Both forecasts work the same if same alpha, beta and gamma arguments are passed, the only difference is how both functions optimize for minimizing error. However, both forecasts showed less than 3% error.

```{r}
forecast(data, h = 12, level = 95, alpha = 0.3146, beta = 0.0070, gamma = 0.5977) %>% plot()
```

Optimizing a smoothing coefficient can be done by simply iterating through all possible variables and comparing errors, for example we will try to find an optimal gamma coefficient only by checking for MAPE while comparing our forecast results to a test set.

```{r}
train = window(data, end = c(1957, 12))
test = window(data, start = c(1958, 1))

gamma = seq(0.01, 0.99, by = 0.01)
MAPE = NA

for (i in seq_along(gamma)) {
     train_ets = ets(train, model = "MAM", gamma = gamma[i])
     train_f = forecast(train_ets, h = 36)
     MAPE[i] = accuracy(train_f, test) [2,5]
}

error = data.frame(gamma, MAPE)
minerr = error[error$MAPE == min(error$MAPE),]

plot(error, type = 'l')
points(minerr, pch = 19, col = 'red')

minerr
```

Now let's plot our new gamma forecast

```{r}
forecast(train, h = 36, gamma = minerr$gamma, level = 95) %>% plot()
lines(test, col = 'red', lty = 2)
```

And check for the model improvement

```{r}
new_forecast = forecast(train, h = 36, gamma = minerr$gamma, level = 95)
old_forecast = forecast(train, h = 36, level = 95)

print("New model MAPE")
accuracy(new_forecast, test) [2, 5]
print("Old model MAPE")
accuracy(old_forecast, test) [2, 5]

if (accuracy(new_forecast, test) [2, 5] < accuracy(old_forecast, test) [2, 5] ) {
    print("Success, we have improved the model!")
} else {
    print("We have not improved the model :( ")
}
```

